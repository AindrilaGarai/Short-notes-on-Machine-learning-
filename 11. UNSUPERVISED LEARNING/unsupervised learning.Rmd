---
title: "Unsupervised Learning"
author: "by AINDRILA GARAI"
date: "MSC STATISTICS, IIT KANPUR aindrilag22@iitk.ac.in"
output: word_document
---

### **Challenge:**

In unsupervised learning, there is no way to check our work because we don’t know the true answer of the problem.

#### **Principal Components Analysis( PCA ):**

Principal components analysis refers to the process by which principal components are computed and the subsequent use of these components in understanding the data.

- PCA is an **unsupervised** approach since it involves only a set of features and no associated response.

- PCA serves as a tool for data visualization and data imputation ( to fill in missing values in a data matrix ).

#### **What Are Principal Components?**

PCA finds a low-dimensional representation of a data set that contains as much as possible of the variation. The first principal component of a set of features $X_1, X_2,...,X_p$ is the normalized linear combination of the features that has the largest variance and each of the variables in X has been centered to have mean zero. The second principal component is the linear combination of $X_1,...,X_p$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_1$.

- A **biplot** is a single display that represents both the principal component scores and the loading vectors.
- Principal components provide low-dimensional linear surfaces that are closest to the observations.
Together the first $M$ principal component score vectors and the first $M$ principal component loading vectors provide the best $M$-dimensional approximation (in terms of Euclidean distance) to the ith observation.

```{r}
# we perform PCA on the USArrests data se
states <- row.names (USArrests)
pr.out <- prcomp (USArrests , scale = TRUE) # centers the variables to have mean zero
names (pr.out)
pr.out$rotation # loading vector
biplot (pr.out , scale = 0 ,cex = 0.4 )
```

### **The Proportion of Variance Explained:**

A natural question is how much of the information in a given data set is lost by projecting the observations onto the first few principal components? Therefore, the PVE of the mth principal component is given by -
$$
\underbrace{\sum_{j=1}^p \frac{1}{n} \sum_{i=1}^n x_{i j}^2}_{\text {Var. of data }}=\underbrace{\sum_{m=1}^M \frac{1}{n} \sum_{i=1}^n z_{i m}^2}_{\text {Var. of first } M \text { PCs }}+\underbrace{\frac{1}{n} \sum_{j=1}^p \sum_{i=1}^n\left(x_{i j}-\sum_{m=1}^M z_{i m} \phi_{j m}\right)^2}_{\text {MSE of } M \text {-dimensional approximation }}
$$
$$\text{PVE} = 1 - \dfrac{\text{RSS}}{TSS}$$
- Each principal component loading vector is unique up to a sign flip.
- In general, a $n × p$ data matrix $X$ has min$(n − 1, p)$ distinct principal components.
- Many statistical techniques, such as regression, classification, and clustering, can be easily adapted to use the $n × M$ matrix whose columns are the first $M ≪ p$ principal component score vectors, rather than using the full $n × p$ data matrix.
-  Principal components can be used to impute impute the missing values through a process known as matrix completion.

```{r}
# To compute the proportion of variance
pr.var <- pr.out$sdev^2
pve <- pr.var / sum (pr.var)
round( pve, 2)
plot (pve , xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot ( cumsum (pve), xlab = " Principal Component ", ylab = " Cumulative Proportion of Variance Explained ", ylim = c(0, 1), type = "b")
```
```{r}
# Matrix Completion
X <- data.matrix ( scale (USArrests))
pcob <- prcomp (X)
sX <- svd (X)
head(t(sX$d * t(sX$u)))
head(pcob$x) # these two are same 
```


### **k-means Clustering Methods:**

When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. $K$-means algorithm finds a local rather than a global optimum. One disadvantage of $K$-means clustering is that it requires us to pre-specify the number of clusters $K$.

```{r}
set.seed (2)
x <- matrix ( rnorm (50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
km.out <- kmeans (x, 5, nstart = 20)
km.out$cluster
par (mfrow = c(1, 2))
plot (x, col = (km.out$cluster + 1),
xlab = "", ylab = "", pch = 20, cex = 2) # K- Means Clustering Results with K = 5
km.out <- kmeans (x, 5, nstart = 1)
km.out$tot.withinss
```

### **Hierarchical Clustering:**

It is a tree-based representation of the observations, called a dendrogram. To group the datasets into clusters, it follows the bottom-up approach. It means, this algorithm considers each dataset as a single cluster at the beginning and then start combining the closet pair of clusters together. It does this until all the clusters together. It dose this until all the clusters are merged into a single cluster that contains all the datasets.

```{r}
hc.complete <- hclust ( dist (x), method = "complete")
hc.average <- hclust ( dist (x), method = "average")
hc.single <- hclust ( dist (x), method = "single")
plot (hc.complete, main = " Complete Linkage ", xlab = "", sub = "", cex = .9)
plot (hc.average , main = " Average Linkage ", xlab = "", sub = "", cex = .9)
plot (hc.single, main = " Single Linkage ", xlab = "", sub = "", cex = .9)
cutree (hc.complete, 2) # To determine the cluster labels
xsc <- scale (x)
plot ( hclust ( dist (xsc), method = "complete") ,
main = " Hierarchical Clustering with Scaled Features ")
```











