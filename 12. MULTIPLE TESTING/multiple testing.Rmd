---
title: "Multiple Testing"
author: "by AINDRILA GARAI"
date: "MSC STATISTICS, IIT KANPUR aindrilag22@iitk.ac.in"
output: word_document
---

#### **Introduction:**

We are often faced with huge amounts of data and consequently may wish to test many null hypotheses. When conducting multiple testing, we need to be very careful about how we interpret the results, in order to avoid erroneously rejecting far too many null hypotheses.

Eg. We might want to test $m$ null hypotheses, $H_{01},...,H_{0m}$, where $H_{0j}$ : the mean value of the jth biomarker among mice in the control group equals the mean value of the jth biomarker among mice in the treatment group.

### **Hypothesis Testing:**

Hypothesis tests provide a rigorous statistical framework for answering simple “yes-or-no” questions about data.

#### **Null and Alternative Hypotheses:**

- First we define the null and alternative hypotheses. Null hypothesis is constructed in such a way so that we can reject it. The alternative hypothesis represents something different and unexpected.

- We use "we fail to reject $H_0$" means $H_0$ really holds or due to small sample size we fail to reject $H_0$ in which case testing $H_0$ again on a larger or higher-quality dataset might lead to rejection.

#### **Test Statistic and p-value:**

- Next, we construct a test statistic that summarizes the strength of evidence against the null hypothesis.

- Test statistics follow a well-known statistical distribution under the null hypothesis — such as a normal distribution, a $t$-distribution, a $\chi^2$-distribution, or an $F$-distribution,  provided that the sample size is sufficiently large.

- A large (absolute) value of a test statistic provides evidence against $H_0$.

- We then compute a p-value that quantifies the probability of having obtained a comparable or more extreme value of the test statistic under the null hypothesis. The p-value is defined as the probability of observing a test statistic equal to or more extreme than the observed statistic, under the assumption that $H_0$ is in fact true.


- Finally, based on the p-value, we decide whether to reject the null hypothesis. A small p-value provides evidence against $H_0$.


**Type I and Type II Errors:**

Type I error occurs when $H_0$ is true but we reject it and Type II error occurs when $H_0$ is false but we do not reject it. The power of the hypothesis test is defined as the probability  of not making a Type II error given that $H_a$ holds, i.e., the probability of correctly rejecting $H_0$.

- Type I and Type II error rates can't be simultaneously small.

- In practice, we typically view Type I errors as more “serious” than Type II errors because the former involves declaring a scientific finding that is not correct.

- **Problem:** Rejecting a null hypothesis if the p-value is below $\alpha$ controls the probability of falsely rejecting that null hypothesis at level $\alpha$. However, if we do this for $m$ null hypotheses, then the chance of falsely rejecting at least one of the m null hypotheses is quite a bit higher.

```{r}
x <- matrix ( rnorm (10 * 100), 10, 100)
x[,1:10] <- x[, 1:10] + 2
t.test (x[, 1], mu = 0) # One Sample t-test

p.values <- rep (0, 100)
for (i in 1:100)
p.values[i] <- t.test (x[, i], mu = 0)$p.value
decision <- rep ("Do not reject H0", 100)
decision[p.values <= .05] <- " Reject H0"
table (decision , c( rep ("H0 is False ", 50), rep ("H0 is True ", 50)))
```
### **The Family-Wise Error Rate:**

The family-wise error rate (FWER) generalizes this notion to the setting of $m$ null hypotheses, $H_{01},...,H_{0m}$, and is defined as the probability of making at least one Type I error.

A strategy of rejecting any null hypothesis for which the p-value is below $\alpha$
$$\operatorname{FWER}(\alpha) = 1-\operatorname{Pr}\left(\bigcap_{j=1}^m\left\{\text { do not falsely reject } H_{0 j}\right\}\right)$$
If two events A and B are independent,
$$\operatorname{FWER}(\alpha)=1-\prod_{j=1}^m(1-\alpha)=1-(1-\alpha)^m$$

### **Approaches to Control the Family-Wise Error Rate:**

**The Bonferroni Method:**
$$\mathrm{FWER}(\alpha) \leq \sum_{j=1}^m \operatorname{Pr}\left(A_j\right)$$
The Bonferroni method sets the threshold for rejecting each hypothesis test to $\alpha / m \text {, so that } \operatorname{Pr}\left(A_j\right) \leq \alpha / m \text {. }$ then,
$\operatorname{FWER}(\alpha) \leq m \times \frac{\alpha}{m}=\alpha$ so this procedure controls the FWER at level $\alpha$

**Holm’s Step-Down Procedure:**

In Holm’s method  it will reject more null hypotheses resulting in fewer Type II errors and hence greater power. Holm’s method makes no independence assumptions about the $m$ hypothesis tests and is uniformly more powerful than the Bonferroni method — it will always reject at least as many null hypotheses as Bonferroni — and so it should always be preferred.


#### **Algorithm:**

1. Specify $\alpha$, the level at which to control the FWER.

2. Compute $p$-values, $p_1,...,p_m$ for the $m$ null hypotheses $H_{01},...,H_{0m}$.

3. Order the $m p$-values so that $p(1) \leq p(2) \leq ···\leq p(m)$.

4. $L=\min \left\{j: p_{(j)}>\frac{\alpha}{m+1-j}\right\}$

5. Reject all null hypotheses $H_{0j}$ for which $p_j < p_{(L)}$.

**Note:**

- Controlling the FWER at level $\alpha$ guarantees that the data analyst is very unlikely (with
probability no more than $\alpha$) to reject any true null hypotheses, i.e. to have any false positives.  When $m$ is large, we may be willing to tolerate a few false positives i.e. more rejections of the null hypothesis.


### **Tukey’s method:**

When performing $m = G(G − 1)/2$ pairwise comparisons of $G$ means, it allows us to control the FWER at level $\alpha$ while rejecting all null hypotheses for which the p-value falls below $\alpha_T$ , for some $\alpha_T > \alpha / m$.

### **Scheff´e’s method:**

It allows us to compute a value $\alpha_S$ such that rejecting the null hypothesis $H_0$ if the
p-value is below $\alpha_S$ will control the Type I error at level $\alpha$.

```{r}
m <- 1:500
fwe1 <- 1 - (1 - 0.05)^m # FWER
fwe2 <- 1 - (1 - 0.01)^m
fwe3 <- 1 - (1 - 0.001)^m
plot (m, fwe1 , type = "l", log = "x", ylim = c(0, 1), col = 2,ylab = " Family - Wise Error Rate ",
xlab = " Number of Hypotheses ")
lines (m, fwe2 , col = 4)
lines (m, fwe3 , col = 3)
abline (h = 0.05, lty = 2)
```
```{r}
library (ISLR2)
fund.mini <- Fund[, 1:5]
t.test (fund.mini[, 1], mu = 0)
fund.pvalue <- rep (0, 5)
for (i in 1:5)
fund.pvalue[i] <- t.test (fund.mini[, i], mu = 0)$p.value
round(fund.pvalue,2)
p.adjust (fund.pvalue , method = "bonferroni") # or "holm"
t.test (fund.mini[, 1], fund.mini[, 2], paired = T) # paired t-test
returns <- as.vector ( as.matrix (fund.mini))
manager <- rep (c("1", "2", "3", "4", "5") , rep (50, 5))
a1 <- aov (returns ~ manager)
a1
TukeyHSD (x = a1) #Tukey multiple comparisons of means
plot ( TukeyHSD (x = a1))
```
### **False Discovery Rate:**

We might try to make sure that the ratio of false positives (V) to total positives (V + S = R) is sufficiently low, so that most of the rejected null hypotheses are not false positives. The ratio V /R is known as the false discovery proportion (FDP).

Controlling the FDP is an impossible task, since one doesn't know which hypotheses are true and which are false. 

So, we define,
$$\mathrm{FDR}=\mathrm{E}(\mathrm{FDP})=\mathrm{E}(V / R)$$
When we control the FDR at (say) level q = 20%, we are rejecting as many null hypotheses as possible while guaranteeing that no more than 20% of those rejected null hypotheses are false positives, on average.

### **The Benjamini-Hochberg Procedure:**

We now focus on the task of controlling the FDR: that is, deciding which null hypotheses to reject while guaranteeing that the FDR, $E(V/R)$, is less than or equal to some pre-specified value q.

1. Specify q, the level at which to control the FDR.

2. Compute p-values, $p_1, \ldots, p_m$ for the $m$ null hypotheses $H_{01}, \ldots, H_{0 m}$.

3. Order the m p-values so that $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$.

4. $L=\max \left\{j: p_{(j)}<q j / m\right\}$.

5. Reject all null hypotheses $H_{0j}$ for which $p_j ≤ p_{(L)}$.
 
The rejection threshold used in the BenjaminiHochberg procedure is more complicated: we reject all null hypotheses for which the p-value is less than or equal to the Lth smallest p-value, where L is itself a function of all m p-values.

```{r}
fund.pvalues <- rep (0, 2000)
for (i in 1:2000)
fund.pvalues[i] <- t.test (Fund[, i], mu = 0)$p.value
q.values.BH <- p.adjust (fund.pvalues , method = "BH")
round(q.values.BH[1:10],3)
sum (q.values.BH <= .1)

ps <- sort (fund.pvalues)
m <- length (fund.pvalues)
q <- 0.1
wh.ps <- which (ps < q * (1:m) / m)
if ( length (wh.ps) >0) {
wh <- 1: max (wh.ps)
} else {
wh <- numeric (0)
}
wh.ps #less than or equal to the largest p-value
```

### **A Re-Sampling Approach:**

If our null hypothesis H0 or test statistic T is somewhat unusual, it may be the case that no theoretical null distribution is available. Even if a theoretical null distribution exists, then we
may be wary of relying upon it, perhaps because some assumption that is required for it to hold is violated or the sample size is too small.

- **Re-Sampling p-Value for a Two-Sample t-Test:**

1. Compute  $T = \dfrac{\hat{\mu}_X-\hat{\mu}_Y}{s \sqrt{\dfrac{1}{n_X}+\dfrac{1}{n_Y}}}$, on the original data $x_1, \ldots, x_{n_X}$ and $y_1, \ldots, y_{n_Y}$

2. For $b = 1,...,B$, where $B$ is a large number (e.g. $B = 10,000$):

(a) Permute the $n_X + n_Y$ observations at random. Call the first $n_X$ permuted observations $x_1^*, \ldots, x_{n_X}^*$ and call the remaining $n_Y$ observations $$y_1^*, \ldots, y_{n_Y}^*$$

(b) Compute $T$ on the permuted data $x_1^*, \ldots, x_{n_X}^*$ and $y_1^*, \ldots, y_{n_Y}^*$ , and call the result $T^{* b}$.

3. The p-value is given by $\dfrac{\sum_{b=1}^B 1_{\left(\left|T^{* b}\right| \geq|T|\right)}}{B}$.

#### **Note:**

- In settings with a smaller sample size or a more skewed data distribution there is a substantial difference between the theoretical and re-sampling null distributions, which results in a difference between their $p$-values.

- The estimated FDR associated with the threshold c is $\widehat{V} / R$. where $R=\sum_{j=1}^m 1_{\left(\left|T^{(j)}\right| \geq c\right)}$ and $\widehat{V}=\frac{\sum_{b=1}^B \sum_{j=1}^m 1\left(\left|T^{(j), * b}\right| \geq c\right)}{B}$ these all are calculated on permuted data.

```{r}
attach (Khan)
x <- rbind (xtrain , xtest)
y <- c(as.numeric (ytrain), as.numeric (ytest))
x <- as.matrix (x)
x1 <- x[ which (y == 2), ]
x2 <- x[ which (y == 4), ]
n1 <- nrow (x1)
n2 <- nrow (x2)
t.out <- t.test (x1[, 11], x2[, 11], var.equal = TRUE)
TT <- t.out$statistic
TT
set.seed (1)
B <- 10000
Tbs <- rep (NA, B)
for (b in 1:B) {
dat <- sample (c(x1[, 11], x2[, 11]))
Tbs[b] <- t.test (dat[1:n1], dat[(n1 + 1):(n1 + n2)],
var.equal = TRUE
)$statistic
 }
mean (( abs (Tbs) >= abs (TT)))
hist (Tbs , breaks = 100, xlim = c(-4.2, 4.2), main = "", xlab = " Null Distribution of Test Statistic ", col = 7)
```






