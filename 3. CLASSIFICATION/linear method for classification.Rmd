---
title: "Linear Methods for Classification"
author: "by AINDRILA GARAI"
date: "MSC STATISTICS, IIT KANPUR aindrilag22@iitk.ac.in"
output: word_document
---

### **INTRODUCTION:**

In this chapter, we study approaches for predicting qualitative responses, a process that is known as classification. Predicting classification a qualitative response for an observation can be referred to as classifying that observation, since it involves assigning the observation to a category or class.
We discuss some widely-used classifiers such as logistic regression, linear discriminant analysis, quadratic discriminant analysis, naive Bayes, and K-nearest neighbors.


### **Why Not Linear Regression:**

There are at least two reasons not to perform classification using a regression method:

(a) A regression method cannot accommodate a qualitative response with more than two classes. 
(b) It will not provide meaningful estimates of $Pr(Y |X)$, even with just two classes.


### **Logistic Regression:**

Logistic regression models the probability that Y belongs to a particular category, then they may choose to use a lower threshold. Here we use the logistic function,
$$p(X)=\dfrac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}$$
To fit the model, we use maximum likelihood method. This above quantity lies between 0 and 1. The logistic function will always produce an S-shaped curve (sigmoid curve).

After a bit of manipulation, we find that 
$$\frac{p(X)}{1-p(X)}=e^{\beta_0+\beta_1 X}$$
The quantity $p(X)/[1−p(X)]$ is called the odds and can take on any value odds between $0$ and $\infty$ indicating very low and very high probabilities respectively.

Next, by taking the logarithm of both sides of the above form, we arrive at
$$\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_0+\beta_1 X$$
The left-hand side is called the log odds or logit which is linear in $X$. The rate of change in $p(X)$ per unit change in $X$ depends on the current value of $X$.

The estimates $\beta_0$ and $\beta_1$ are chosen to maximize this likelihood function.

In general, we take
$$\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_0+\beta_1 X_1+\cdots+\beta_p X_p$$
where $X = (X_1,...,X_p)$ are $p$ predictors.

```{r}
library (ISLR2)
data (Smarket)
pairs (Smarket,lwd=1) #  to plot Scatterplot matrix
cor(Smarket[, -9]) # removing the direction qualitative variable
```


```{r}
glm.fits <- glm (Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial) # in order to tell R to run a logistic regression
summary (glm.fits)
```

```{r}
glm.probs <- predict (glm.fits , type = "response")
glm.probs [11:15]
contrasts (Smarket$Direction)
glm.pred <- rep ("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
table (glm.pred ,Smarket$Direction)
mean (glm.pred == Smarket$Direction)
```

```{r}
train <- (Smarket$Year < 2005) # true or false output
Smarket.2005 <- Smarket[!train , ] # give a submatrix
Direction.2005 <- Smarket$Direction[!train] # ! symbol can be used to reverse all of the elements
```

```{r}
glm.fits <- glm ( Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume , data = Smarket , family = binomial , subset = train)
glm.probs <- predict (glm.fits , Smarket.2005, type = "response")
glm.fits
```

```{r}
glm.pred <- rep (" Down ", 252)
glm.pred[glm.probs > .5] <- "Up"
table (glm.pred , Direction.2005)
```
```{r}
glm.fits <- glm (Direction ~ Lag1 + Lag2 , data = Smarket , family = binomial , subset = train)
predict (glm.fits, newdata = data.frame (Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), type = "response")
```
### **Multinomial Logistic Regression:**

We sometimes wish to classify a response variable that has more than two classes. The extension of the two-class logistic regression approach into the setting of $K > 2$ classes is known as multinomial logistic regression. The mathematical form is -

$$\operatorname{Pr}(Y=k \mid X=x)=\frac{e^{\beta_{k 0}+\beta_{k 1} x_1+\cdots+\beta_{k p} x_p}}{1+\sum_{l=1}^{K-1} e^{\beta_{l 0}+\beta_{l 1} x_1+\cdots+\beta_{l p} x_p}}$$
$\text { for } k=1, \ldots, K-1$


### **Generative Models for Classification:**

**Reasons to use:**

- When there is substantial separation between the two classes.
- Each of the classes and the sample size are small & more than two response classes.

Let $\pi_k$ represents the prior probability that a randomly chosen observation comes from the prior kth class. Let $f_k(X) ≡ Pr(X|Y = k)$ denote the density function of X. Then Bayes’ theorem states that-

$$\operatorname{Pr}(Y=k \mid X=x)=\frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}$$

Instead of directly computing the posterior probability $p_k(x)$, we can simply plug in estimates of $\pi_k$
and $f_k(X)$. In general, to estimate $\pi_k$ we compute the fraction of the training observations that belong to the kth class.


### **Linear Discriminant Analysis:**

In particular, we assume that $f_k(x)$ is normal or Gaussian.where $\mu_k$ and $\sigma_k^2$ are the mean and variance parameters for the kth class. Further assume that $\sigma^2_1 = ··· = \sigma^2_k=\sigma_k^2$.Then,

$$p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2}\left(x-\mu_k\right)^2\right)}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2}\left(x-\mu_l\right)^2\right)}$$
where $\pi_k$ denotes the prior probability that an observation belongs to the kth class,

$$\delta_k(x)=x \cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2 \sigma^2}+\log \left(\pi_k\right)$$
Now, we assign the observation to the class for which it is largest.

If X is drawn from a Gaussian distribution within each class, to apply the Bayes classifier we still have to estimate the parameters $\mu_1,...,\mu_K$, $\pi_1,..., \pi_K$ and $σ^2$. The linear discriminant analysis (LDA) method approximates the Bayes classifier. In particular, the following estimates are used-

$$\begin{aligned}
\hat{\mu}_k & =\frac{1}{n_k} \sum_{i: y_i=k} x_i \\
\hat{\sigma}^2 & =\frac{1}{n-K} \sum_{k=1}^K \sum_{i: y_i=k}\left(x_i-\hat{\mu}_k\right)^2
\end{aligned}$$

where $n$ is the total number of training observations, and $n_k$ is the number of training observations in the kth class.

In the case of p > 1 predictors, the LDA classifier assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution $N(\mu_k, \sum)$, where $µ_k$ is a class-specific mean vector, and $\sum$ is a covariance matrix that is common to all K classes.Again, Bayes classifier assigns an observation $X = x$ to the class for which is largest

$$\delta_k(x)=x^T \boldsymbol{\Sigma}^{-1} \mu_k-\frac{1}{2} \mu_k^T \boldsymbol{\Sigma}^{-1} \mu_k+\log \pi_k$$
```{r}
library (MASS)
lda.fit <- lda (Direction ~ Lag1 + Lag2 , data = Smarket , subset = train)
lda.fit
lda.pred <- predict (lda.fit , Smarket.2005)
lda.class <- lda.pred$class
sum (lda.pred$posterior[, 1] < .5)
```

**Remark:**

- The LDA decision boundaries are pretty close to the Bayes decision boundaries impling that LDA is performing well on this data.

- The higher the ratio of parameters $p$ to number of samples $n$, the more we expect this overfitting to play a role.


- **confusion matrix:** 

A binary classifier can make two types of errors: 

1. It can incorrectly assign an individual who defaults to the no default category
2. It can incorrectly assign an individual who does not default to the default category. 

A confusion matrix is a convenient way to display this assigning to default category information.


- **low sensitivity:**

LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers i.e. the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors term. Varying the classifier threshold changes its true
positive and false positive rate. These are also called the sensitivity.

- **ROC curve:** The ROC (receiver operating characteristic) curve is a popular graphic for simultaneously displaying two types of errors for all possible thresholds. The overall performance of a classifier summarized over all possible thresholds is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier.


 
**Quadratic Discriminant Analysis:**

The QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form $X ∼ N(\mu_k, \sum_k)$, where $\sum_k$ is a covariance matrix for the kth class.Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which is largest.

$$\begin{aligned}
\delta_k(x) & =-\frac{1}{2}\left(x-\mu_k\right)^T \boldsymbol{\Sigma}_k^{-1}\left(x-\mu_k\right)-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_k\right|+\log \pi_k \\
& =-\frac{1}{2} x^T \boldsymbol{\Sigma}_k^{-1} x+x^T \boldsymbol{\Sigma}_k^{-1} \mu_k-\frac{1}{2} \mu_k^T \boldsymbol{\Sigma}_k^{-1} \mu_k-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_k\right|+\log \pi_k
\end{aligned}$$

```{r}
qda.fit <- qda (Direction ~ Lag1 + Lag2 , data = Smarket , subset = train)
qda.fit
```

**What to choose LDA OR QDA?**

The answer lies in the bias-variance trade-of. LDA is a much less flexible classifier than QDA, and has lower variance. This can potentially lead to improved prediction performance. If LDA’s assumption that
the K classes share a common covariance matrix is off, then LDA can suffer from high bias. LDA tends to be a better if there are relatively few training observations.


### **Naive Bayes:**

We assume here for $k = 1,...,K$

$$f_k(x)=f_{k 1}\left(x_1\right) \times f_{k 2}\left(x_2\right) \times \cdots \times f_{k p}\left(x_p\right)$$
where $f_{kj}$ is the density function of the jth predictor among observations in the kth class.we completely eliminate the association between the $p$ predictors. The naive Bayes assumption introduces some bias but reduces variance. Here,

$$\operatorname{Pr}(Y=k \mid X=x)=\frac{\pi_k \times f_{k 1}\left(x_1\right) \times f_{k 2}\left(x_2\right) \times \cdots \times f_{k p}\left(x_p\right)}{\sum_{l=1}^K \pi_l \times f_{l 1}\left(x_1\right) \times f_{l 2}\left(x_2\right) \times \cdots \times f_{l p}\left(x_p\right)}$$
for $k = 1,...,K$

- If $X_j$ is quantitative, then we can assume that $X_j |Y = k \sim N(\mu_{jk}, \sigma^2_{jk})$. QDA with
an additional assumption that the class-specific covariance matrix is diagonal.

- We can use a kernel density estimator, which is essentially a smoothed version of a histogram.

- If $X_j$ is qualitative, then we can simply count the proportion of training observations for the jth predictor corresponding to each class.

- LDA is a special case of QDA with $c_{kjl}$ = 0 for all $j = 1,...,p$, $l = 1,...,p$, and $k = 1,...,K$. (since LDA is simply a restricted version of QDA with

$\boldsymbol{\Sigma}_1=\cdots=\boldsymbol{\Sigma}_K=\boldsymbol{\Sigma}$.)

```{r}
library (e1071)
nb.fit <- naiveBayes (Direction ~ Lag1 + Lag2 , data = Smarket , subset = train)
nb.fit
nb.class <- predict (nb.fit , Smarket.2005)
table (nb.class , Direction.2005)
```

### **K-Nearest Neighbors:**

```{r}
library (class)
train.X <- cbind (Smarket$Lag1 , Smarket$Lag2)[train , ]
test.X <- cbind (Smarket$Lag1 , Smarket$Lag2)[!train , ]

```



