---
title: "Moving Beyond Linearity"
author: "by AINDRILA GARAI"
date: "MSC STATISTICS, IIT KANPUR aindrilag22@iitk.ac.in"
output: word_document
---

### **Polynomial Regression:**

The nonlinear relationship between the predictors and the response is defined by a polynomial function $$y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\cdots+\beta_dx_i^d+\epsilon_i$$
where $\epsilon_i$ is the error term.

- Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power.

- Least squares returns variance estimates for each of the fitted coefficients $\beta_j^2$ as well as the covariances between pairs of coefficient estimates.

```{r}
library (ISLR2)
attach (Wage)
fit <- lm(wage ~ poly (age , 4), data = Wage)
coef ( summary (fit))
fit2 <- lm(wage ~ poly (age , 4, raw = T), data = Wage)
coef ( summary (fit2))
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)
coef (fit2a)
fit2b <- lm(wage ~ cbind (age , age^2, age^3, age^4), data = Wage)
agelims <- range (age)
age.grid <- seq (from = agelims[1], to = agelims [2])
preds <- predict (fit , newdata = list (age = age.grid), se = TRUE)
se.bands <- cbind (preds$fit + 2 * preds$se.fit , preds$fit - 2 * preds$se.fit)
library(ggplot2)
ggplot()+
  geom_point(aes(Wage$age , Wage$wage),col="pink")+
  geom_smooth(aes(Wage$age , Wage$wage),col="red")
  labs(title = "Degree -4 Polynomial" , x= "age" , y="Wage")
preds2 <- predict (fit2 , newdata = list (age = age.grid), se = TRUE)
max (abs (preds$fit - preds2$fit))
```

```{r}
fit <- glm (I(wage > 250) ~ poly (age , 4), data = Wage , family = binomial)
preds <- predict (fit , newdata = list (age = age.grid), se = T)
pfit <- exp (preds$fit) / (1 + exp (preds$fit))
se.bands.logit <- cbind (preds$fit + 2 * preds$se.fit ,
preds$fit - 2 * preds$se.fit)
se.bands <- exp (se.bands.logit) / (1 + exp (se.bands.logit))
preds <- predict (fit , newdata = list (age = age.grid), type = "response", se = T)
plot (age , I(wage > 250), xlim = agelims , type = "n", ylim = c(0, .2))
points ( jitter (age), I((wage > 250) / 5), cex = .5, pch = "|", col = "darkgrey")
lines (age.grid, pfit , lwd = 2, col = "blue")
matlines (age.grid , se.bands , lwd = 1, col = "blue", lty = 3)
table ( cut (age , 4))
fit <- lm(wage ~ cut (age , 4), data = Wage)
coef(summary (fit))
```
### **Step Functions:**

Step functions cut the range of a variable into K distinct regions in order to produce a qualitative variable. We create cutpoints $c_1, c_2,...,c_K$ in the range of $X$, and then construct $K + 1$ new variables.
$$
\begin{array}{ll}
C_0(X) & =I\left(X<c_1\right) \\
C_1(X) & =I\left(c_1 \leq X<c_2\right) \\
C_2(X) & =I\left(c_2 \leq X<c_3\right) \\
& \vdots \\
C_{K-1}(X) & =I\left(c_{K-1} \leq X<c_K\right) \\
C_K(X) & =I\left(c_K \leq X\right)
\end{array}
$$
We then use least squares to fit the linear model
$$y_i=\beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i)+\beta_3C_3(x_i)+\cdots+\beta_KC_K(x_i)+\epsilon_i$$

### **Regression Splines:**

- Regression splines are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of $X$ into $K$ distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are constrained so that they join smoothly at the region boundaries, or knots.

A piecewise cubic polynomial with a single knot at a point $c$ takes the form
$$y_i= \begin{cases}\beta_{01}+\beta_{11} x_i+\beta_{21} x_i^2+\beta_{31} x_i^3+\epsilon_i & \text { if } x_i<c \\ \beta_{02}+\beta_{12} x_i+\beta_{22} x_i^2+\beta_{32} x_i^3+\epsilon_i & \text { if } x_i \geq c .\end{cases}$$
- To choose number of knots an approach is to use cross-validation. With this method, we remove a portion of the data (say 10 %), fit a spline with a certain number of knots to the remaining data and then use the spline to make predictions for the held-out portion.

```{r}
library (splines)
fit <- lm(wage ~ bs(age , knots = c(25, 40, 60)), data = Wage)
pred <- predict (fit , newdata = list (age = age.grid), se = T)
plot (age , wage , col = "gray")
lines (age.grid, pred$fit , lwd = 2)
lines (age.grid , pred$fit + 2 * pred$se, lty = "dashed")
lines (age.grid , pred$fit - 2 * pred$se, lty = "dashed")
fit2 <- lm(wage ~ ns(age , df = 4), data = Wage)
pred2 <- predict (fit2 , newdata = list (age = age.grid), se = T)
lines (age.grid , pred2$fit , col = "red", lwd = 2)
plot (age , wage , xlim = agelims , cex = .5, col = "darkgrey")
title ("Smoothing Spline")
fit <- smooth.spline (age , wage , df = 16)
fit2 <- smooth.spline (age , wage , cv = TRUE)
fit2$df
lines (fit , col = "red", lwd = 2)
lines (fit2 , col = "blue", lwd = 2)
legend ("topright", legend = c("16 DF", "6.8 DF") , col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```

### **Smoothing Splines:**

 Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty. We really want is a function $g$ that makes RSS small but that is also smooth. A natural approach is to find the function g that minimizes
$$
\sum_{i=1}^n\left(y_i-g\left(x_i\right)\right)^2+\lambda \int g^{\prime \prime}(t)^2 d t
$$
where $\lambda$ is a nonnegative tuning parameter and chosen using LOOCV. Ihe equation takes the “Loss+Penalty” formulation.


### **Local Regression:**

- In Local regression the regions are allowed to overlap, and indeed they do so in a very
smooth way.

1. Gather the fraction $s = k/n$ of training points whose xi are closest to $x_0$.

2. Assign a weight $K_{i0} = K(x_i, x_0)$ to each point in this neighborhood, so that the point furthest from $x_0$ has weight zero, and the closest has the highest weight. All but these $k$ nearest neighbors get weight zero.

3. Fit a weighted least squares regression of the $y_i$ on the $x_i$ using the aforementioned weights, by finding $\hat{\beta_0}$ and $\hat{\beta_1}$ that minimize $\sum_{i=1}^n K_{i 0}\left(y_i-\beta_0-\beta_1 x_i\right)^2$.

4. The fitted value at $x_0$ is given by $\hat{f}\left(x_0\right)=\hat{\beta}_0+\hat{\beta}_1 x_0$

```{r}
plot (age , wage , xlim = agelims , cex = .5, col = "darkgrey")
title ("Local Regression")
fit <- loess (wage ~ age , span = .2, data = Wage)
fit2 <- loess (wage ~ age , span = .5, data = Wage)
lines (age.grid, predict (fit , data.frame (age = age.grid)),
col = "red", lwd = 2)
lines (age.grid, predict (fit2 , data.frame (age = age.grid)),
col = "blue", lwd = 2)
legend ("topright", legend = c("Span = 0.2", "Span = 0.5") ,
col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```

### **Generalized Additive Models:**

Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.

```{r}
gam1 <- lm(wage ~ ns(year , 4) + ns(age , 5) + education , data = Wage)
library (gam)
gam.m3 <- gam (wage ~ s(year , 4) + s(age , 5) + education , data = Wage)
par (mfrow = c(1, 3))
plot (gam.m3, se = TRUE , col = "blue")
plot.Gam (gam1 , se = TRUE , col = "red")
gam.m1 <- gam (wage ~ s(age , 5) + education , data = Wage)
gam.m2 <- gam (wage ~ year + s(age , 5) + education , data = Wage)
anova (gam.m1 , gam.m2 , gam.m3 , test = "F")
```

















